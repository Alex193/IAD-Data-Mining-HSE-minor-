{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport numpy as np\nimport time\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:05:58.708367Z","iopub.execute_input":"2021-12-20T18:05:58.709017Z","iopub.status.idle":"2021-12-20T18:06:01.070710Z","shell.execute_reply.started":"2021-12-20T18:05:58.708917Z","shell.execute_reply":"2021-12-20T18:06:01.069976Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"–ó–∞–¥–∞–Ω–∏–µ 0 (0 –±–∞–ª–ª–æ–≤, –Ω–æ —Å–¥–µ–ª–∞—Ç—å –Ω—É–∂–Ω–æ)\n–ü—Ä–æ—Å—Ç–∞–≤—å—Ç–µ –∑–Ω–∞–∫–∏ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤, –∏—Å—Ö–æ–¥—è –∏–∑ –≤–∞—à–µ–≥–æ –æ–ø—ã—Ç–∞:\nùëÉ(—Ä–∞–º—É|–º–∞–º–∞,–º—ã–ª–∞)‚àóùëÉ(–ø–∞–ø—É|–º–∞–º–∞,–º—ã–ª–∞) \nùëÉ(—Å—Ç–æ–ª—É|–¥–æ—Ä–æ–≥–∞,–ª–æ–∂–∫–∞,–∫)‚àóùëÉ(–æ–±–µ–¥—É|–¥–æ—Ä–æ–≥–∞,–ª–æ–∂–∫–∞,–∫) \nùëÉ(–ï–≤–ø–∞—Ç–∏–π|–º–µ–Ω—è,–∑–æ–≤—É—Ç)‚àóùëÉ(–í–∞–Ω—è|–º–µ–Ω—è,–∑–æ–≤—É—Ç) \nùëÉ(–∂—É—Ä–Ω–∞–ª—ã|—è,—á–∞—Å—Ç–æ,—á–∏—Ç–∞—é)‚àóùëÉ(–∫–æ–º–∏–∫—Å—ã|—è,—á–∞—Å—Ç–æ,—á–∏—Ç–∞—é) \n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±—ä—è—Å–Ω–∏—Ç—å –≤—ã–±–æ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ –ø—Ä–∏–º–µ—Ä–æ–≤.","metadata":{}},{"cell_type":"markdown","source":"*–û—Ç–≤–µ—Ç –Ω–∞ –∑–∞–¥–∞–Ω–∏–µ 0: *\n1) ùëÉ(—Ä–∞–º—É|–º–∞–º–∞,–º—ã–ª–∞) > ùëÉ(–ø–∞–ø—É|–º–∞–º–∞,–º—ã–ª–∞) –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ —Å–ª–æ–≤–æ \"—Ä–∞–º–∞\" —á–∞—â–µ —É–ø–æ—Ç—Ä–µ–±–ª—è–µ—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ø–æ—Å–ª–µ —Å–ª–æ–≤ \"–º–∞–º–∞, –º—ã–ª–∞\" –Ω–∞—á–∞–ª–∞ —Ä—É—Å—Å–∫–æ–π —Å–∫–æ—Ä–æ–≥–æ–≤–æ—Ä–∫–∏ \"–º–∞–º–∞ –º—ã–ª–∞ —Ä–∞–º—É...\", –º—ã –¥–æ–≤–æ–ª—å–Ω–æ —Ä–µ–¥–∫–æ –º–æ–∂–µ–º –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å —Ä—è–¥–æ–º —Å–ª–æ–≤–∞ \"–º–∞–º–∞ –º—ã–ª–∞ –ø–∞–ø—É\" –∏–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ–¥–æ–±–Ω–æ–µ.\n\n2) ùëÉ(—Å—Ç–æ–ª—É|–¥–æ—Ä–æ–≥–∞,–ª–æ–∂–∫–∞,–∫) < ùëÉ(–æ–±–µ–¥—É|–¥–æ—Ä–æ–≥–∞,–ª–æ–∂–∫–∞,–∫) –¢—É—Ç –ø–æ—Ö–æ–∂–∞—è —Å–∏—Ç—É–∞—Ü–∏—è –∫–∞–∫ —Å –ø–µ—Ä–≤—ã–º –ø—Ä–∏–º–µ—Ä–æ–º. –ï—Å—Ç—å —Ä—É—Å—Å–∫–∞—è –ø–æ–≥–æ–≤–æ—Ä–∫–∞ \"—Ö–æ—Ä–æ—à–∞ –ª–æ–∂–∫–∞ –∫ –æ–±–µ–¥—É\", —Ç–∞–∫ –º—ã –≥–æ–≤–æ—Ä–∏–º —á–∞—â–µ, —á–µ–º \"–¥–æ—Ä–æ–≥–∞ –ª–æ–∂–∫–∞ –∫ —Å—Ç–æ–ª—É\", –≤—Ç–æ—Ä–∞—è —Ñ—Ä–∞–∑–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–≥–æ–≤–æ—Ä–∫–æ–π.\n\n3) ùëÉ(–ï–≤–ø–∞—Ç–∏–π|–º–µ–Ω—è,–∑–æ–≤—É—Ç) < (–í–∞–Ω—è|–º–µ–Ω—è,–∑–æ–≤—É—Ç) –ò–º—è –í–∞–Ω—è –±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–∞—Å—Ç—Ä–∞–Ω–µ–Ω–Ω–æ–µ, —á–µ–º –ï–≤–ø–∞—Ç–∏–π, –ø–æ—ç—Ç–æ–º—É —á–∞—â–µ –ø–æ –∂–∏–∑–Ω–∏ –Ω–∞–º –±—É–¥–µ—Ç –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è —Ñ—Ä–∞–∑–∞ \"–º–µ–Ω—è –∑–æ–≤—É—Ç –ï–≤–ø–∞—Ç–∏–π\".\n\n4) ùëÉ(–∂—É—Ä–Ω–∞–ª—ã|—è,—á–∞—Å—Ç–æ,—á–∏—Ç–∞—é) = ùëÉ(–∫–æ–º–∏–∫—Å—ã|—è,—á–∞—Å—Ç–æ,—á–∏—Ç–∞—é) –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ —Ç—É—Ç –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞, –æ—Ç —á—å–µ–≥–æ –∏–º–µ–Ω–∏ –ø—Ä–æ–∏–∑–Ω–æ—Å–∏—Ç—Å—è/–ø–∏—à–µ—Ç—Å—è –¥–∞–Ω–Ω–∞—è —Ñ—Ä–∞–∑–∞. –ö–æ–º–∏–∫—Å—ã –∏ –∂—É—Ä–Ω–∞–ª—ã –∫–∞–∂–µ—Ç—Å—è –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ –ø–æ–ø—É–ª—è—Ä–Ω—ã, –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ —Å–∫–∞–∑–∞—Ç—å –∫–∞–∫–∞—è —Ñ—Ä–∞–∑–∞ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —á–∞—â–µ —Å–ª–æ–∂–Ω–æ, –ø–æ—ç—Ç–æ–º—É —Ç—É—Ç –Ω–µ–ª—å–∑—è –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∑–Ω–∞–∫ <>, —Å–∫–æ—Ä–µ–µ –æ–Ω–∏ —Ä–∞–≤–Ω—ã.","metadata":{}},{"cell_type":"code","source":"\"\"\"–ò—Å—Ç–æ—á–Ω–∏–∫–∏: https://pytorch.org/tutorials/beginner/transformer_tutorial.html, –º–∞–ª–µ–Ω—å–∫–∏–µ –∫—É—Å–∫–∏ –∫–æ–¥–∞ –∏–∑ —Å–µ–º–∏–Ω–∞—Ä–æ–≤\"\"\"\n# —á–∏—Ç–∞–µ–º —Ñ–∞–π–ª\ndirectory = '/kaggle/input/hse-ida-transformers/small_corp_for_test.txt'\nfile = open(directory, 'r')\ndata = file.readlines()\nfile.close()\n# –≤—ã–≤–æ–¥–∏–º –∫–æ–ª–∏—á–µ—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ —Ñ–∞–π–ª–µ\nprint(f\"Total lines in text: {len(data)}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:06:01.072492Z","iopub.execute_input":"2021-12-20T18:06:01.072744Z","iopub.status.idle":"2021-12-20T18:06:01.979551Z","shell.execute_reply.started":"2021-12-20T18:06:01.072711Z","shell.execute_reply":"2021-12-20T18:06:01.978773Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Total lines in text: 700000\n","output_type":"stream"}]},{"cell_type":"code","source":"# –∫–∞–∫–∏–µ —Å–∏–º–≤–æ–ª—ã –≤—Ö–æ–¥—è—Ç –≤ —Ç–µ–∫—Å—Ç?\nlist_of_all_letters = []\nfor words in data:\n  for letter in words:\n    list_of_all_letters.append(letter)\ncollection = Counter(list_of_all_letters)\nsorted(collection.items(), key=lambda item:item[1], reverse=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:06:01.980737Z","iopub.execute_input":"2021-12-20T18:06:01.980984Z","iopub.status.idle":"2021-12-20T18:06:09.418712Z","shell.execute_reply.started":"2021-12-20T18:06:01.980949Z","shell.execute_reply":"2021-12-20T18:06:09.417966Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"[(' ', 4380220),\n ('–æ', 2779376),\n ('–∞', 2366842),\n ('–µ', 2028774),\n ('—Ç', 1985934),\n ('–Ω', 1582468),\n ('–∏', 1501660),\n ('—Å', 1401746),\n ('–≤', 1130062),\n ('—Ä', 1055739),\n ('–ª', 915552),\n ('–¥', 899550),\n ('–º', 878018),\n ('—É', 848660),\n ('–∫', 795405),\n ('–ø', 757949),\n ('\\n', 700000),\n ('—è', 555131),\n ('—å', 538119),\n ('—ã', 403604),\n ('–≥', 402608),\n ('–±', 384499),\n ('—á', 370993),\n ('–∑', 355889),\n ('–π', 289713),\n ('–∂', 249344),\n ('—à', 187256),\n ('—ç', 162631),\n ('—é', 149791),\n ('—Ö', 147781),\n ('—Ü', 117824),\n ('—ë', 76849),\n ('—â', 66697),\n ('—Ñ', 60354),\n ('-', 43618),\n ('—ä', 5406)]"},"metadata":{}}]},{"cell_type":"code","source":"# —Å–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ?\nletter_counter = sum([elem for elem in collection.values()])\nprint(f\"–í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–∞—Ö –¥–æ —É–¥–∞–ª–µ–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤ –ø–µ—Ä–µ–Ω–æ—Å–∞ –Ω–∞ –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É –∏ —Ç–∞–±—É–ª—è—Ü–∏–∏: {letter_counter}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:06:09.420646Z","iopub.execute_input":"2021-12-20T18:06:09.420885Z","iopub.status.idle":"2021-12-20T18:06:09.427699Z","shell.execute_reply.started":"2021-12-20T18:06:09.420852Z","shell.execute_reply":"2021-12-20T18:06:09.427004Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"–í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–∞—Ö –¥–æ —É–¥–∞–ª–µ–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤ –ø–µ—Ä–µ–Ω–æ—Å–∞ –Ω–∞ –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É –∏ —Ç–∞–±—É–ª—è—Ü–∏–∏: 30576062\n","output_type":"stream"}]},{"cell_type":"code","source":"# —É–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª—ã –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏ –∏ —Ç–∞–±—É–ª—è—Ü–∏\ndata = [line.rstrip(\"\\n\").rstrip(\"\\t\") for line in data]\nprint(\"–í–æ—Ç —Ç–∞–∫ —Å–µ–π—á–∞—Å –≤—ã–≥–ª—è–¥—è—Ç –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ: \")\nprint(data[:25])","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:06:09.428815Z","iopub.execute_input":"2021-12-20T18:06:09.429207Z","iopub.status.idle":"2021-12-20T18:06:09.668027Z","shell.execute_reply.started":"2021-12-20T18:06:09.429170Z","shell.execute_reply":"2021-12-20T18:06:09.667324Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"–í–æ—Ç —Ç–∞–∫ —Å–µ–π—á–∞—Å –≤—ã–≥–ª—è–¥—è—Ç –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ: \n['–¥–æ–±—Ä–æ', '–∫–æ–≥–æ', '–∫–∞–ø–∏—Ç–∞–Ω', '–Ω–µ—Ç', '–∑–∞—á–µ–º', '—á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç', '—á—Ç–æ —Ç–∞–∫–æ–µ', '—Ä–∞—Å—Å–∫–∞–∑', '–Ω–∏–∫–æ–º—É', '–Ω—É —á—Ç–æ', '–∫—Ç–æ', '—è —É–∫–∞–∂—É', '–∏—Å–ø–æ–ª–Ω—è–π', '–∂–¥–µ—Ç', '–æ–Ω –¥—É–º–∞–ª', '–≤ –±—Ä–æ–Ω–µ', '–æ—Ç–µ—Ü', '–±—ã—Å—Ç—Ä–æ', '–Ω—É –∂–∏–ª', '—á—Ç–æ', '–∑–¥–æ—Ä–æ–≤', '—á—Ç–æ —Å —Ç–æ–±–æ–π', '–∏–æ—Ä—Ç—ç', '–∫–∞–∫', '–Ω—É –∫–∞–∫']\n","output_type":"stream"}]},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self):\n        self.alphabet = '_–¥–æ–±—Å—Ä–∫–≥–∞—É–ø–∏—Ç–Ω–µ–∑—á–º—Ñ—è–∂–ª–π–≤—Ü—ã—ç—å-—à—Ö—é—â—ë—ä][ '\n        self.token2ind = {}\n        self.ind2token = {}\n        for i in range(len(self.alphabet)):\n            self.token2ind[self.alphabet[i]] = i\n            self.ind2token[i] = self.alphabet[i]\n        \n    \n    def preprocess(self, text, window_size):\n        list_of_batches = []\n        for word in text.lower():\n          list_of_batches.append(self.token2ind[word])\n        first_v = list_of_batches[1:] + [0] * (window_size - len(list_of_batches[1:]))\n        second_v = list_of_batches + [0] * (window_size - len(list_of_batches))\n        return first_v, second_v","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:06:09.669422Z","iopub.execute_input":"2021-12-20T18:06:09.669667Z","iopub.status.idle":"2021-12-20T18:06:10.098593Z","shell.execute_reply.started":"2021-12-20T18:06:09.669633Z","shell.execute_reply":"2021-12-20T18:06:10.096022Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# –¥–æ–±–∞–≤–∏–º '[' –∏ ']' –≤ –Ω–∞—à —Ç–µ–∫—Å—Ç\nfor i in range(len(data)):\n  data[i] = '[' + data[i] + ']' \n# –≤—ã–≤–æ–¥–∏–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —á—Ç–æ –ø–æ–ª—É—á–∏–ª–æ—Å—å\nprint(data[25:31])","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:06:10.101615Z","iopub.execute_input":"2021-12-20T18:06:10.101857Z","iopub.status.idle":"2021-12-20T18:06:10.329275Z","shell.execute_reply.started":"2021-12-20T18:06:10.101807Z","shell.execute_reply":"2021-12-20T18:06:10.328512Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['[–Ω—É —á—Ç–æ –∂]', '[–∞—Ö –¥–∞]', '[—Å—É–¥–∞—Ä—å]', '[—É—Ç–æ–Ω—É–ª]', '[–º–µ–Ω—è]', '[—Å–ø—Ä–æ—Å–∏–ª –æ–Ω]']\n","output_type":"stream"}]},{"cell_type":"code","source":"THRESHOLD = 128\n# —É–¥–∞–ª—è–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã –∏–∑ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\ncount_before = len(data)\ndata_batched = [phrase for phrase in data if len(phrase) <= 128]\ncount_after = len(data_batched)\nprint(f'–ë—ã–ª–æ —É–¥–∞–ª–µ–Ω–æ {count_before - count_after} —Ñ—Ä–∞–∑')\n# —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ç–µ—Å—Ç–æ–≤—É—é –∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –≤—ã–±–æ—Ä–∫–∏\ndata_train, data_test = train_test_split(data_batched, test_size= int(0.15 * len(data_batched)), random_state=42, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:06:10.330622Z","iopub.execute_input":"2021-12-20T18:06:10.332057Z","iopub.status.idle":"2021-12-20T18:06:10.690549Z","shell.execute_reply.started":"2021-12-20T18:06:10.331932Z","shell.execute_reply":"2021-12-20T18:06:10.689844Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"–ë—ã–ª–æ —É–¥–∞–ª–µ–Ω–æ 16562 —Ñ—Ä–∞–∑\n","output_type":"stream"}]},{"cell_type":"code","source":"# –≤–Ω–∏–∑ –¥–∞–ª—å—à–µ –≤–µ—Ä–Ω–æ\nfrom sys import prefix\nclass TextDataset(torch.utils.data.Dataset):\n    def __init__(self, x, preproc, win_size = 128):\n        self.x = x\n        self.preproc = preproc\n        self.win_size = win_size\n    \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, idx):\n        first_vec, second_vec = preproc.preprocess(self.x[idx], self.win_size)\n        first_vec = torch.tensor(first_vec)\n        second_vec = torch.tensor(second_vec)\n        return first_vec, second_vec","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:06:10.692579Z","iopub.execute_input":"2021-12-20T18:06:10.692789Z","iopub.status.idle":"2021-12-20T18:06:10.699198Z","shell.execute_reply.started":"2021-12-20T18:06:10.692764Z","shell.execute_reply":"2021-12-20T18:06:10.698033Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# —Å–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –∫–ª–∞—Å—Å–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä\npreproc = Preprocessor()\n# –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç, –ø–æ–ª—É—á–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –≤—ã–±–æ—Ä–∫–∏ —É–∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ\ntrain_dataset = TextDataset(data_train, preproc)\ntest_dataset = TextDataset(data_test, preproc)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:06:10.700467Z","iopub.execute_input":"2021-12-20T18:06:10.700939Z","iopub.status.idle":"2021-12-20T18:06:10.709732Z","shell.execute_reply.started":"2021-12-20T18:06:10.700902Z","shell.execute_reply":"2021-12-20T18:06:10.709022Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"–ò–°–¢–û–ß–ù–ò–ö: https://habr.com/ru/company/wunderfund/blog/594333/\n\n–î–∞–Ω–Ω—ã–π –ø—Ä–∏–µ–º –Ω–µ–æ–±—Ö–¥–∏–º –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è. –°–º—ã—Å–ª –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤ –≤ —Ç–æ–º, —á—Ç–æ–±—ã –Ω–µ –¥–∞—Ç—å –º–æ–¥–µ–ª–∏ \"–º–æ—á—å –≤–∏–¥–µ—Ç—å\" —á—Ç–æ –±—É–¥–µ—Ç –≤ –±—É–¥—É—â–µ–º. –ö –º–∞—Ç—Ä–∏—Ü–µ —Å–∫–∞–ª—è—Ä–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π –º—ã –ø—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É, —Ç–æ –µ—Å—Ç—å –æ—Ç–∫–ª—é—á–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –≤—ã—à–µ –≥–ª–∞–≤–Ω–æ–π –¥–∏–∞–≥–æ–Ω–∞–ª–∏. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤—ã—Ö–æ–¥ –∑–∞–≤–∏—Å–∏—Ç –Ω–µ –æ—Ç –≤—Å–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ü–µ–ª–∏–∫–æ–º, –∞ —Ç–æ–ª—å–∫–æ –æ—Ç n-1 –ø—Ä–µ–¥—É–¥—ã—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤, –µ—Å–ª–∏ –º—ã –Ω–∞—Ö–æ–¥–∏–º—Å—è –≤ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –Ω–∞ n-–æ–º —Å–∏–º–≤–æ–ª–µ.","metadata":{}},{"cell_type":"code","source":"import math\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:06:10.711151Z","iopub.execute_input":"2021-12-20T18:06:10.711613Z","iopub.status.idle":"2021-12-20T18:06:10.763789Z","shell.execute_reply.started":"2021-12-20T18:06:10.711564Z","shell.execute_reply":"2021-12-20T18:06:10.763014Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from torch.nn import TransformerEncoder, TransformerEncoderLayer\nclass LanguageModel(nn.Module):\n    def __init__(self, vocab_size): # vocab_size - —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è\n        super(LanguageModel, self).__init__()\n        self.hidden_size = 64\n        self.linear_size = 1024\n        self.emb = nn.Embedding(vocab_size, self.hidden_size) # —ç–º–±–µ–¥–¥–∏–Ω–≥\n        self.pe = PositionalEncoding(d_model=self.hidden_size) \n        # —ç–Ω–∫–æ–¥–µ—Ä\n        self.transformer_encoder_layer = TransformerEncoderLayer(d_model=self.hidden_size, nhead=8) \n        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=6)\n        self.decoder = nn.Sequential(\n            nn.Linear(self.hidden_size, self.linear_size),\n            nn.ReLU(),\n            nn.Linear(self.linear_size, vocab_size)\n        )\n    \n    def forward(self, x, src_mask):\n        x = self.emb(x) # emb, then pe\n        x = self.pe(x)\n        x = x.transpose(1, 0)\n        # print(x.shape, src_mask.shape)\n        x = self.transformer_encoder(x, src_mask) # transformer encoder with mask\n        x = self.decoder(x).transpose(2,1) # decoder\n        return x.transpose(2, 0)\n    \n    def generate_square_subsequent_mask(self, sz):\n        # –ê –≤–æ—Ç –∏ —Ç–æ —Å–∞–º–æ–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:06:10.766849Z","iopub.execute_input":"2021-12-20T18:06:10.767073Z","iopub.status.idle":"2021-12-20T18:06:10.779068Z","shell.execute_reply.started":"2021-12-20T18:06:10.767039Z","shell.execute_reply":"2021-12-20T18:06:10.778365Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(len('_–¥–æ–±—Å—Ä–∫–≥–∞—É–ø–∏—Ç–Ω–µ–∑—á–º—Ñ—è–∂–ª–π–≤—Ü—ã—ç—å-—à—Ö—é—â—ë—ä][ '))","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:06:10.780478Z","iopub.execute_input":"2021-12-20T18:06:10.780725Z","iopub.status.idle":"2021-12-20T18:06:10.911870Z","shell.execute_reply.started":"2021-12-20T18:06:10.780693Z","shell.execute_reply":"2021-12-20T18:06:10.911222Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import tqdm\nclass Trainer:\n    def __init__(self, model, train_dataset, test_dataset):\n        \n        self.model = model\n        \n        self.train_batch_size = 64\n        self.test_batch_size = 64\n        \n        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, self.train_batch_size)\n        self.test_dataloader = torch.utils.data.DataLoader(test_dataset, self.test_batch_size)\n        self.train_dataloader_size = len(self.train_dataloader)\n        self.test_dataloader_size = len(self.test_dataloader)\n        self.device = 'cuda:0'\n        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n        self.steps_to_print = 100\n        \n    def train_one_epoch(self, epoch_number):\n        step = 0\n        counted_loss = 0\n        current_time = time.time()\n        it = 0\n        for batch in self.train_dataloader:\n            x, y = batch\n            # print(x.__getitem__(5))\n            x, y = x.to(self.device), y.to(self.device)\n            sequence_length = x.size(1)\n            # print(sequence_length)\n            # src_mask = self.model.generate_square_subsequent_mask(train_dataset.win_size).to(self.device)\n            src_mask = self.model.generate_square_subsequent_mask(sequence_length).to(self.device)\n            # print(x.shape, src_mask.shape)\n            predicted = self.model(x, src_mask)\n            loss = self.criterion(predicted, y)\n            counted_loss += loss.detach().item()\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n            step += 1\n            it += 1\n            if step%self.steps_to_print == 0:\n                result = 'Train epoch '+str(epoch_number)+' | '\n                result += 'Step '+str(step)+'/'+str(self.train_dataloader_size)+' | '\n                result += 'Counted loss '+str(counted_loss)+' | '\n                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n                result += 'time '+str(time.time() - current_time) + ' | '\n                print(result)\n                current_time = time.time()\n                counted_loss = 0\n                it = 0\n    \n    def validate_one_epoch(self, epoch_number):\n        step = 0\n        counted_loss = 0\n        current_time = time.time()\n        it = 0\n        for batch in self.test_dataloader:\n            x, y = batch\n            x, y = x.to(self.device), y.to(self.device)\n            sequence_length = y.size(1)\n            predicted = self.model(x, model.generate_square_subsequent_mask(sequence_length).to(self.device))\n            loss = self.criterion(predicted, y)\n            counted_loss += loss.item()\n            step += 1\n            it += 1\n            # YOUR CODE HERE\n            \n            # —Ä–µ–∞–ª–∏–∑—É–π—Ç–µ —à–∞–≥–∏ –¥–ª—è —Ç–µ—Å—Ç–∞ –º–æ–¥–µ–ª–∏\n            # –ø–æ–º–Ω–∏—Ç–µ, —á—Ç–æ –¥–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —É–∂–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∏–∑ \n            # –±–ª–æ–∫–∞ with torch.no_grad(), –∞ –ø–æ—Ç–æ–º—É \n            # –ø–æ–≤—Ç–æ—Ä–Ω–æ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ\n            \n            ################\n            \n            if step%(self.steps_to_print//2) == 0:\n                result = 'Validate epoch '+str(epoch_number)+' | '\n                result += 'Step '+str(step)+'/'+str(self.test_dataloader_size)+' | '\n                result += 'Counted loss '+str(counted_loss)+' | '\n                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n                result += 'time '+str(time.time() - current_time) + ' | '\n                print(result)\n                current_time = time.time()\n                counted_loss = 0\n                it = 0\n        \n    def train(self, number_of_epochs):\n        model.to(self.device)\n        for epoch in range(1, number_of_epochs+1):\n            model.train()\n            self.train_one_epoch(epoch)\n            with torch.no_grad():\n                model.eval()\n                self.validate_one_epoch(epoch)\n            print()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:06:10.913924Z","iopub.execute_input":"2021-12-20T18:06:10.914332Z","iopub.status.idle":"2021-12-20T18:06:11.440426Z","shell.execute_reply.started":"2021-12-20T18:06:10.914297Z","shell.execute_reply":"2021-12-20T18:06:11.439508Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model = model, train_dataset = train_dataset, test_dataset = test_dataset)\ntrainer.train(number_of_epochs = 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:06:21.080913Z","iopub.execute_input":"2021-12-20T18:06:21.081190Z","iopub.status.idle":"2021-12-20T18:16:15.699473Z","shell.execute_reply.started":"2021-12-20T18:06:21.081159Z","shell.execute_reply":"2021-12-20T18:16:15.698683Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Train epoch 1 | Step 100/9077 | Counted loss 285.1531136035919 | ppl 17.3142720612153 | time 7.3764495849609375 | \nTrain epoch 1 | Step 200/9077 | Counted loss 218.22955441474915 | ppl 8.866636671031873 | time 6.133527040481567 | \nTrain epoch 1 | Step 300/9077 | Counted loss 184.04751229286194 | ppl 6.299530601533901 | time 6.089079141616821 | \nTrain epoch 1 | Step 400/9077 | Counted loss 151.71138882637024 | ppl 4.559048266536202 | time 6.25527286529541 | \nTrain epoch 1 | Step 500/9077 | Counted loss 108.43766486644745 | ppl 2.9575956228267817 | time 6.140430450439453 | \nTrain epoch 1 | Step 600/9077 | Counted loss 75.8302121758461 | ppl 2.134648768185245 | time 6.09723424911499 | \nTrain epoch 1 | Step 700/9077 | Counted loss 55.74238455295563 | ppl 1.7461683017548204 | time 6.137481212615967 | \nTrain epoch 1 | Step 800/9077 | Counted loss 43.90214928984642 | ppl 1.5511886265690846 | time 6.090056896209717 | \nTrain epoch 1 | Step 900/9077 | Counted loss 35.49918235838413 | ppl 1.4261689932826058 | time 6.16962194442749 | \nTrain epoch 1 | Step 1000/9077 | Counted loss 30.218805596232414 | ppl 1.352815607830767 | time 6.1660401821136475 | \nTrain epoch 1 | Step 1100/9077 | Counted loss 26.23633074760437 | ppl 1.2999987559117137 | time 6.134219408035278 | \nTrain epoch 1 | Step 1200/9077 | Counted loss 22.28156788647175 | ppl 1.2495902266185068 | time 6.090415954589844 | \nTrain epoch 1 | Step 1300/9077 | Counted loss 20.25863790512085 | ppl 1.2245658573904987 | time 6.144371747970581 | \nTrain epoch 1 | Step 1400/9077 | Counted loss 18.243587344884872 | ppl 1.2001371878271503 | time 6.180837154388428 | \nTrain epoch 1 | Step 1500/9077 | Counted loss 16.350630186498165 | ppl 1.1776327764467933 | time 6.202456712722778 | \nTrain epoch 1 | Step 1600/9077 | Counted loss 14.564714431762695 | ppl 1.1567879368313392 | time 6.158595323562622 | \nTrain epoch 1 | Step 1700/9077 | Counted loss 13.793769404292107 | ppl 1.1479040267870084 | time 6.09765362739563 | \nTrain epoch 1 | Step 1800/9077 | Counted loss 12.57799270004034 | ppl 1.134032570869695 | time 6.1838929653167725 | \nTrain epoch 1 | Step 1900/9077 | Counted loss 11.55656822770834 | ppl 1.1225082410243068 | time 6.155636787414551 | \nTrain epoch 1 | Step 2000/9077 | Counted loss 11.333526618778706 | ppl 1.1200073706107305 | time 6.276740789413452 | \nTrain epoch 1 | Step 2100/9077 | Counted loss 10.147497616708279 | ppl 1.1068022216115314 | time 6.195309638977051 | \nTrain epoch 1 | Step 2200/9077 | Counted loss 9.818112086504698 | ppl 1.1031625727745724 | time 6.16165828704834 | \nTrain epoch 1 | Step 2300/9077 | Counted loss 8.932074457406998 | ppl 1.0934313122483537 | time 6.216981410980225 | \nTrain epoch 1 | Step 2400/9077 | Counted loss 8.529655154794455 | ppl 1.089039975306959 | time 6.1732177734375 | \nTrain epoch 1 | Step 2500/9077 | Counted loss 8.05704678595066 | ppl 1.0839052244322034 | time 6.204597473144531 | \nTrain epoch 1 | Step 2600/9077 | Counted loss 7.852661792188883 | ppl 1.0816921471761645 | time 6.223484992980957 | \nTrain epoch 1 | Step 2700/9077 | Counted loss 7.4241896867752075 | ppl 1.0770673131963717 | time 6.151123285293579 | \nTrain epoch 1 | Step 2800/9077 | Counted loss 7.309883080422878 | ppl 1.0758368574825874 | time 6.111494779586792 | \nTrain epoch 1 | Step 2900/9077 | Counted loss 6.9651706628501415 | ppl 1.0721346988083134 | time 6.170330762863159 | \nTrain epoch 1 | Step 3000/9077 | Counted loss 6.424372300505638 | ppl 1.0663522616705974 | time 6.1684346199035645 | \nTrain epoch 1 | Step 3100/9077 | Counted loss 6.325891364365816 | ppl 1.0653026249116415 | time 6.235138893127441 | \nTrain epoch 1 | Step 3200/9077 | Counted loss 5.72684608772397 | ppl 1.0589400562157878 | time 6.17704701423645 | \nTrain epoch 1 | Step 3300/9077 | Counted loss 5.7648424953222275 | ppl 1.0593424918464682 | time 6.155012607574463 | \nTrain epoch 1 | Step 3400/9077 | Counted loss 5.343420762568712 | ppl 1.0548875859255176 | time 6.171689510345459 | \nTrain epoch 1 | Step 3500/9077 | Counted loss 5.093593947589397 | ppl 1.0522554830863207 | time 6.097162246704102 | \nTrain epoch 1 | Step 3600/9077 | Counted loss 5.344228032976389 | ppl 1.0548961017552059 | time 6.138382434844971 | \nTrain epoch 1 | Step 3700/9077 | Counted loss 4.880066046491265 | ppl 1.0500110211680953 | time 6.17955207824707 | \nTrain epoch 1 | Step 3800/9077 | Counted loss 4.934133592993021 | ppl 1.0505788898677417 | time 6.141018867492676 | \nTrain epoch 1 | Step 3900/9077 | Counted loss 4.5255126766860485 | ppl 1.0462947636603592 | time 6.145402669906616 | \nTrain epoch 1 | Step 4000/9077 | Counted loss 4.619363501667976 | ppl 1.047277180859011 | time 6.100466966629028 | \nTrain epoch 1 | Step 4100/9077 | Counted loss 4.341627258807421 | ppl 1.0443725480407184 | time 6.140994071960449 | \nTrain epoch 1 | Step 4200/9077 | Counted loss 4.058077454566956 | ppl 1.041415426163139 | time 6.184208631515503 | \nTrain epoch 1 | Step 4300/9077 | Counted loss 3.992583639919758 | ppl 1.0407335867799037 | time 6.158517837524414 | \nTrain epoch 1 | Step 4400/9077 | Counted loss 4.057490115985274 | ppl 1.041409309546508 | time 6.089126110076904 | \nTrain epoch 1 | Step 4500/9077 | Counted loss 3.930143415927887 | ppl 1.0400839532345867 | time 6.144662618637085 | \nTrain epoch 1 | Step 4600/9077 | Counted loss 3.7200458608567715 | ppl 1.0379010561877764 | time 6.119343519210815 | \nTrain epoch 1 | Step 4700/9077 | Counted loss 3.6661921460181475 | ppl 1.0373422583928609 | time 6.129519701004028 | \nTrain epoch 1 | Step 4800/9077 | Counted loss 3.6594038661569357 | ppl 1.0372718430872636 | time 6.2311084270477295 | \nTrain epoch 1 | Step 4900/9077 | Counted loss 3.478263646364212 | ppl 1.035394627300538 | time 6.089036703109741 | \nTrain epoch 1 | Step 5000/9077 | Counted loss 3.532534711062908 | ppl 1.0359566994960938 | time 6.147552490234375 | \nTrain epoch 1 | Step 5100/9077 | Counted loss 3.423324601724744 | ppl 1.0348259476119486 | time 6.082247734069824 | \nTrain epoch 1 | Step 5200/9077 | Counted loss 3.1026922427117825 | ppl 1.0315132743536468 | time 6.146457195281982 | \nTrain epoch 1 | Step 5300/9077 | Counted loss 2.9596012365072966 | ppl 1.0300383271403089 | time 6.206220865249634 | \nTrain epoch 1 | Step 5400/9077 | Counted loss 2.964201772585511 | ppl 1.0300857155152197 | time 6.1481053829193115 | \nTrain epoch 1 | Step 5500/9077 | Counted loss 2.9042483707889915 | ppl 1.0294683291778968 | time 6.156486988067627 | \nTrain epoch 1 | Step 5600/9077 | Counted loss 2.9272718131542206 | ppl 1.0297053755123993 | time 6.102669715881348 | \nTrain epoch 1 | Step 5700/9077 | Counted loss 2.8456849921494722 | ppl 1.0288656142446466 | time 6.150707244873047 | \nTrain epoch 1 | Step 5800/9077 | Counted loss 2.8466710122302175 | ppl 1.028875759116222 | time 6.180020093917847 | \nTrain epoch 1 | Step 5900/9077 | Counted loss 2.7718132846057415 | ppl 1.0281058543054442 | time 6.175442934036255 | \nTrain epoch 1 | Step 6000/9077 | Counted loss 2.5938438680022955 | ppl 1.0262777675146793 | time 6.0896642208099365 | \nTrain epoch 1 | Step 6100/9077 | Counted loss 2.470755693502724 | ppl 1.025015318067731 | time 6.136595249176025 | \nTrain epoch 1 | Step 6200/9077 | Counted loss 2.5262289848178625 | ppl 1.0255840855435814 | time 6.084482908248901 | \nTrain epoch 1 | Step 6300/9077 | Counted loss 2.5975885996595025 | ppl 1.0263161995827146 | time 6.157453775405884 | \nTrain epoch 1 | Step 6400/9077 | Counted loss 2.5746067482978106 | ppl 1.0260803602203894 | time 6.235499382019043 | \nTrain epoch 1 | Step 6500/9077 | Counted loss 2.498088590800762 | ppl 1.0252955227441913 | time 6.089648246765137 | \nTrain epoch 1 | Step 6600/9077 | Counted loss 2.296560615301132 | ppl 1.0232313460796505 | time 6.144521236419678 | \nTrain epoch 1 | Step 6700/9077 | Counted loss 2.4731584079563618 | ppl 1.0250399465548052 | time 6.089536190032959 | \nTrain epoch 1 | Step 6800/9077 | Counted loss 2.3784593120217323 | ppl 1.0240697024711458 | time 6.153265714645386 | \nTrain epoch 1 | Step 6900/9077 | Counted loss 2.3414581082761288 | ppl 1.0236908544475243 | time 6.2129175662994385 | \nTrain epoch 1 | Step 7000/9077 | Counted loss 2.1604121793061495 | ppl 1.0218391805262141 | time 6.169154167175293 | \nTrain epoch 1 | Step 7100/9077 | Counted loss 2.1864586416631937 | ppl 1.022105368148444 | time 6.0948474407196045 | \nTrain epoch 1 | Step 7200/9077 | Counted loss 2.1279353434219956 | ppl 1.021507373375703 | time 6.1377623081207275 | \nTrain epoch 1 | Step 7300/9077 | Counted loss 2.064419292844832 | ppl 1.0208587582449433 | time 6.179247617721558 | \nTrain epoch 1 | Step 7400/9077 | Counted loss 2.1276544267311692 | ppl 1.0215045037950237 | time 6.2010650634765625 | \nTrain epoch 1 | Step 7500/9077 | Counted loss 2.0265715094283223 | ppl 1.0204724589406076 | time 6.134411334991455 | \nTrain epoch 1 | Step 7600/9077 | Counted loss 1.9377661691978574 | ppl 1.0195666271745092 | time 6.097852945327759 | \nTrain epoch 1 | Step 7700/9077 | Counted loss 1.9321255190297961 | ppl 1.0195091186097838 | time 6.140524864196777 | \nTrain epoch 1 | Step 7800/9077 | Counted loss 1.9595457669347525 | ppl 1.0197887088679394 | time 6.090453386306763 | \nTrain epoch 1 | Step 7900/9077 | Counted loss 1.8477431191131473 | ppl 1.0186491952111574 | time 6.151960134506226 | \nTrain epoch 1 | Step 8000/9077 | Counted loss 1.8252864442765713 | ppl 1.0184204661570946 | time 6.201310634613037 | \nTrain epoch 1 | Step 8100/9077 | Counted loss 1.7916198130697012 | ppl 1.018077656004171 | time 6.131364822387695 | \nTrain epoch 1 | Step 8200/9077 | Counted loss 1.793874045368284 | ppl 1.0181006060981905 | time 6.147805452346802 | \nTrain epoch 1 | Step 8300/9077 | Counted loss 1.9331562141887844 | ppl 1.019519626695068 | time 6.095240116119385 | \nTrain epoch 1 | Step 8400/9077 | Counted loss 1.7907437505200505 | ppl 1.0180687370461683 | time 6.152536153793335 | \nTrain epoch 1 | Step 8500/9077 | Counted loss 1.7579795429483056 | ppl 1.0177352295304702 | time 6.181857347488403 | \nTrain epoch 1 | Step 8600/9077 | Counted loss 1.762027058750391 | ppl 1.0177764233583666 | time 6.130357980728149 | \nTrain epoch 1 | Step 8700/9077 | Counted loss 1.607665357645601 | ppl 1.0162065782893166 | time 6.0940468311309814 | \nTrain epoch 1 | Step 8800/9077 | Counted loss 1.6464565666392446 | ppl 1.0166008535740387 | time 6.141027450561523 | \nTrain epoch 1 | Step 8900/9077 | Counted loss 1.7407808084972203 | ppl 1.0175602070021896 | time 6.125002861022949 | \nTrain epoch 1 | Step 9000/9077 | Counted loss 1.5567848798818886 | ppl 1.0156896590438593 | time 6.206652402877808 | \nValidate epoch 1 | Step 50/1602 | Counted loss 0.181096363812685 | ppl 1.0036284943809826 | time 1.023925542831421 | \nValidate epoch 1 | Step 100/1602 | Counted loss 0.19557184173027053 | ppl 1.003919096487154 | time 0.9955649375915527 | \nValidate epoch 1 | Step 150/1602 | Counted loss 0.20725724135991186 | ppl 1.0041537478227978 | time 0.9914124011993408 | \nValidate epoch 1 | Step 200/1602 | Counted loss 0.24400922621134669 | ppl 1.0048921120196217 | time 0.9920790195465088 | \nValidate epoch 1 | Step 250/1602 | Counted loss 0.1958396506961435 | ppl 1.0039244736722555 | time 1.0016629695892334 | \nValidate epoch 1 | Step 300/1602 | Counted loss 0.19536675815470517 | ppl 1.003914978749241 | time 0.9926357269287109 | \nValidate epoch 1 | Step 350/1602 | Counted loss 0.2079677612055093 | ppl 1.0041680173475025 | time 0.9941563606262207 | \nValidate epoch 1 | Step 400/1602 | Counted loss 0.206397840869613 | ppl 1.0041364885666602 | time 0.9928650856018066 | \nValidate epoch 1 | Step 450/1602 | Counted loss 0.23945470858598128 | ppl 1.0048005802118016 | time 0.9906809329986572 | \nValidate epoch 1 | Step 500/1602 | Counted loss 0.22494172066217288 | ppl 1.0045089693615674 | time 0.9959516525268555 | \nValidate epoch 1 | Step 550/1602 | Counted loss 0.2028401418356225 | ppl 1.0040650428002083 | time 1.0144767761230469 | \nValidate epoch 1 | Step 600/1602 | Counted loss 0.16887725720880553 | ppl 1.0033832554769355 | time 1.032583475112915 | \nValidate epoch 1 | Step 650/1602 | Counted loss 0.2256373808486387 | ppl 1.0045229453967315 | time 0.9969244003295898 | \nValidate epoch 1 | Step 700/1602 | Counted loss 0.2113615993584972 | ppl 1.0042361793353696 | time 0.9926776885986328 | \nValidate epoch 1 | Step 750/1602 | Counted loss 0.19449834118131548 | ppl 1.0038975425645125 | time 0.9932653903961182 | \nValidate epoch 1 | Step 800/1602 | Counted loss 0.20865940931253135 | ppl 1.0041819080617438 | time 0.9924478530883789 | \nValidate epoch 1 | Step 850/1602 | Counted loss 0.23928400897420943 | ppl 1.0047971498362782 | time 1.004831075668335 | \nValidate epoch 1 | Step 900/1602 | Counted loss 0.19140462658833712 | ppl 1.0038354290366018 | time 1.0039680004119873 | \nValidate epoch 1 | Step 950/1602 | Counted loss 0.20021194824948907 | ppl 1.004012266641169 | time 0.9942746162414551 | \nValidate epoch 1 | Step 1000/1602 | Counted loss 0.19502483418909833 | ppl 1.0039081135209016 | time 0.9941387176513672 | \nValidate epoch 1 | Step 1050/1602 | Counted loss 0.20953430858207867 | ppl 1.0041994793758318 | time 0.9979753494262695 | \nValidate epoch 1 | Step 1100/1602 | Counted loss 0.19540316180791706 | ppl 1.0039157096729618 | time 1.0207741260528564 | \nValidate epoch 1 | Step 1150/1602 | Counted loss 0.18783453048672527 | ppl 1.0037637558163979 | time 1.0249593257904053 | \nValidate epoch 1 | Step 1200/1602 | Counted loss 0.2037216259050183 | ppl 1.004082744303041 | time 0.9922666549682617 | \nValidate epoch 1 | Step 1250/1602 | Counted loss 0.1855805265950039 | ppl 1.003718507088074 | time 0.9967644214630127 | \nValidate epoch 1 | Step 1300/1602 | Counted loss 0.21900406532222405 | ppl 1.0043896878833116 | time 0.9930174350738525 | \nValidate epoch 1 | Step 1350/1602 | Counted loss 0.19746477936860174 | ppl 1.0039571043314859 | time 0.9942219257354736 | \nValidate epoch 1 | Step 1400/1602 | Counted loss 0.18560400366550311 | ppl 1.0037189783755878 | time 1.0732262134552002 | \nValidate epoch 1 | Step 1450/1602 | Counted loss 0.18843717605341226 | ppl 1.0037758541648567 | time 1.0124948024749756 | \nValidate epoch 1 | Step 1500/1602 | Counted loss 0.23355988203547895 | ppl 1.0046821246919224 | time 0.9997594356536865 | \nValidate epoch 1 | Step 1550/1602 | Counted loss 0.22439568443223834 | ppl 1.004497999455656 | time 0.9998073577880859 | \nValidate epoch 1 | Step 1600/1602 | Counted loss 0.20572619407903403 | ppl 1.0041230001562578 | time 0.9998917579650879 | \n\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_text(text):\n    with torch.no_grad():\n        model.eval()\n        x = []\n        device = 'cuda:0'\n        for letter in text:\n            x.append(preproc.token2ind[letter])\n        x = torch.from_numpy(np.array(x)).to(device)\n        sequence_length = len(x)\n        src_mask = model.generate_square_subsequent_mask(sequence_length).to(device)\n        pred = model(x, src_mask)\n        ind = pred.topk(1)[1].view(-1)[-1].item()\n        text += preproc.ind2token[ind]  \n        if len(text) > 150 or text[-1] == \"]\":\n            return text\n        else:\n            return generate_text(text)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:17:05.102430Z","iopub.execute_input":"2021-12-20T18:17:05.102685Z","iopub.status.idle":"2021-12-20T18:17:05.109675Z","shell.execute_reply.started":"2021-12-20T18:17:05.102657Z","shell.execute_reply":"2021-12-20T18:17:05.108879Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"generate_text('[–æ–≥–æ –∏–∞–¥ ')","metadata":{"execution":{"iopub.status.busy":"2021-12-20T18:22:34.620432Z","iopub.execute_input":"2021-12-20T18:22:34.620684Z","iopub.status.idle":"2021-12-20T18:22:37.066627Z","shell.execute_reply.started":"2021-12-20T18:22:34.620655Z","shell.execute_reply":"2021-12-20T18:22:37.065928Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'[–æ–≥–æ –∏–∞–¥ —Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—É—É—É—É—É—É—É—É—É—Ä—Ä—Ä—Ä—É—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä—Ä'"},"metadata":{}}]},{"cell_type":"markdown","source":"–ù–µ —Ö–≤–∞—Ç–∏–ª–æ –≤—Ä–µ–º–µ–º–∏, —á—Ç–æ–±—ã –ø–æ–¥–æ–±—Ä–∞—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –æ–±—É—á–∏—Ç—å –Ω–∞ –Ω—É–∂–Ω–æ–º —á–∏—Å–ª–µ —ç–ø–æ—Ö, —ç—Ç–∏–º –æ–±—ä—è—Å–Ω—è–µ—Ç—Å—è —Ç–∞–∫–∞—è —Å—Ç—Ä–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}