{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8.775_loss.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_upCOEI3Upu"
      },
      "source": [
        "# Основы глубинного обучения, майнор ИАД\n",
        "\n",
        "## Домашнее задание 1. Введение в PyTorch. Полносвязные нейронные сети.\n",
        "\n",
        "### Общая информация\n",
        "\n",
        "Дата выдачи: 06.10.2021\n",
        "\n",
        "Мягкий дедлайн: 23:59MSK 25.10.2021\n",
        "\n",
        "Жесткий дедлайн: 23:59MSK 28.10.2021\n",
        "\n",
        "### Оценивание и штрафы\n",
        "Максимально допустимая оценка за работу — 10 баллов. За каждый день просрочки снимается 1 балл. Сдавать задание после жёсткого дедлайна сдачи нельзя.\n",
        "\n",
        "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
        "\n",
        "Неэффективная реализация кода может негативно отразиться на оценке.\n",
        "Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
        "\n",
        "### О задании\n",
        "\n",
        "В этом задании вам предстоит предсказывать год выпуска песни по некоторым звуковым признакам: [данные](https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd). В ячейках ниже находится код для загрузки данных. Обратите внимание, что обучающая и тестовая выборки располагаются в одном файле, поэтому НЕ меняйте ячейку, в которой производится деление данных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI_eoe063VaP"
      },
      "source": [
        "# импорт библиотек\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NgSZeU-7vgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a875a4-c18b-4188-971a-6dfb3b901247"
      },
      "source": [
        "# скачиваем данные \n",
        "!wget -O data.txt.zip https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-26 16:11:34--  https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 211011981 (201M) [application/x-httpd-php]\n",
            "Saving to: ‘data.txt.zip’\n",
            "\n",
            "data.txt.zip        100%[===================>] 201.24M  55.4MB/s    in 4.0s    \n",
            "\n",
            "2021-10-26 16:11:38 (50.8 MB/s) - ‘data.txt.zip’ saved [211011981/211011981]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSVJZzkJ7zZE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "0cee0152-9061-4609-fef4-51e84f42a7de"
      },
      "source": [
        "# считываем данные\n",
        "df = pd.read_csv('data.txt.zip', header=None)\n",
        "# выводим первые записи посмотреть структуру данных\n",
        "df.head(10)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>-2.46783</td>\n",
              "      <td>3.32136</td>\n",
              "      <td>-2.31521</td>\n",
              "      <td>10.20556</td>\n",
              "      <td>611.10913</td>\n",
              "      <td>951.08960</td>\n",
              "      <td>698.11428</td>\n",
              "      <td>408.98485</td>\n",
              "      <td>383.70912</td>\n",
              "      <td>326.51512</td>\n",
              "      <td>238.11327</td>\n",
              "      <td>251.42414</td>\n",
              "      <td>187.17351</td>\n",
              "      <td>100.42652</td>\n",
              "      <td>179.19498</td>\n",
              "      <td>-8.41558</td>\n",
              "      <td>-317.87038</td>\n",
              "      <td>95.86266</td>\n",
              "      <td>48.10259</td>\n",
              "      <td>-95.66303</td>\n",
              "      <td>-18.06215</td>\n",
              "      <td>1.96984</td>\n",
              "      <td>34.42438</td>\n",
              "      <td>11.72670</td>\n",
              "      <td>1.36790</td>\n",
              "      <td>7.79444</td>\n",
              "      <td>-0.36994</td>\n",
              "      <td>-133.67852</td>\n",
              "      <td>-83.26165</td>\n",
              "      <td>-37.29765</td>\n",
              "      <td>...</td>\n",
              "      <td>-25.38187</td>\n",
              "      <td>-3.90772</td>\n",
              "      <td>13.29258</td>\n",
              "      <td>41.55060</td>\n",
              "      <td>-7.26272</td>\n",
              "      <td>-21.00863</td>\n",
              "      <td>105.50848</td>\n",
              "      <td>64.29856</td>\n",
              "      <td>26.08481</td>\n",
              "      <td>-44.59110</td>\n",
              "      <td>-8.30657</td>\n",
              "      <td>7.93706</td>\n",
              "      <td>-10.73660</td>\n",
              "      <td>-95.44766</td>\n",
              "      <td>-82.03307</td>\n",
              "      <td>-35.59194</td>\n",
              "      <td>4.69525</td>\n",
              "      <td>70.95626</td>\n",
              "      <td>28.09139</td>\n",
              "      <td>6.02015</td>\n",
              "      <td>-37.13767</td>\n",
              "      <td>-41.12450</td>\n",
              "      <td>-8.40816</td>\n",
              "      <td>7.19877</td>\n",
              "      <td>-8.60176</td>\n",
              "      <td>-5.90857</td>\n",
              "      <td>-12.32437</td>\n",
              "      <td>14.68734</td>\n",
              "      <td>-54.32125</td>\n",
              "      <td>40.14786</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>4.59210</td>\n",
              "      <td>2.21920</td>\n",
              "      <td>0.34006</td>\n",
              "      <td>44.38997</td>\n",
              "      <td>2056.93836</td>\n",
              "      <td>605.40696</td>\n",
              "      <td>457.41175</td>\n",
              "      <td>777.15347</td>\n",
              "      <td>415.64880</td>\n",
              "      <td>746.47775</td>\n",
              "      <td>366.45320</td>\n",
              "      <td>317.82946</td>\n",
              "      <td>273.07917</td>\n",
              "      <td>141.75921</td>\n",
              "      <td>317.35269</td>\n",
              "      <td>19.48271</td>\n",
              "      <td>-65.25496</td>\n",
              "      <td>162.75145</td>\n",
              "      <td>135.00765</td>\n",
              "      <td>-96.28436</td>\n",
              "      <td>-86.87955</td>\n",
              "      <td>17.38087</td>\n",
              "      <td>45.90742</td>\n",
              "      <td>32.49908</td>\n",
              "      <td>-32.85429</td>\n",
              "      <td>45.10830</td>\n",
              "      <td>26.84939</td>\n",
              "      <td>-302.57328</td>\n",
              "      <td>-41.71932</td>\n",
              "      <td>-138.85034</td>\n",
              "      <td>...</td>\n",
              "      <td>28.55107</td>\n",
              "      <td>1.52298</td>\n",
              "      <td>70.99515</td>\n",
              "      <td>-43.63073</td>\n",
              "      <td>-42.55014</td>\n",
              "      <td>129.82848</td>\n",
              "      <td>79.95420</td>\n",
              "      <td>-87.14554</td>\n",
              "      <td>-45.75446</td>\n",
              "      <td>-65.82100</td>\n",
              "      <td>-43.90031</td>\n",
              "      <td>-19.45705</td>\n",
              "      <td>12.59163</td>\n",
              "      <td>-407.64130</td>\n",
              "      <td>42.91189</td>\n",
              "      <td>12.15850</td>\n",
              "      <td>-88.37882</td>\n",
              "      <td>42.25246</td>\n",
              "      <td>46.49209</td>\n",
              "      <td>-30.17747</td>\n",
              "      <td>45.98495</td>\n",
              "      <td>130.47892</td>\n",
              "      <td>13.88281</td>\n",
              "      <td>-4.00055</td>\n",
              "      <td>17.85965</td>\n",
              "      <td>-18.32138</td>\n",
              "      <td>-87.99109</td>\n",
              "      <td>14.37524</td>\n",
              "      <td>-22.70119</td>\n",
              "      <td>-58.81266</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>1.39518</td>\n",
              "      <td>2.73553</td>\n",
              "      <td>0.82804</td>\n",
              "      <td>7.46586</td>\n",
              "      <td>699.54544</td>\n",
              "      <td>1016.00954</td>\n",
              "      <td>594.06748</td>\n",
              "      <td>355.73663</td>\n",
              "      <td>507.39931</td>\n",
              "      <td>387.69910</td>\n",
              "      <td>287.15347</td>\n",
              "      <td>112.37152</td>\n",
              "      <td>161.68928</td>\n",
              "      <td>144.14353</td>\n",
              "      <td>199.29693</td>\n",
              "      <td>-4.24359</td>\n",
              "      <td>-297.00587</td>\n",
              "      <td>-148.36392</td>\n",
              "      <td>-7.94726</td>\n",
              "      <td>-18.71630</td>\n",
              "      <td>12.77542</td>\n",
              "      <td>-25.37725</td>\n",
              "      <td>9.71410</td>\n",
              "      <td>0.13843</td>\n",
              "      <td>26.79723</td>\n",
              "      <td>6.30760</td>\n",
              "      <td>28.70107</td>\n",
              "      <td>-74.89005</td>\n",
              "      <td>-289.19553</td>\n",
              "      <td>-166.26089</td>\n",
              "      <td>...</td>\n",
              "      <td>18.50939</td>\n",
              "      <td>16.97216</td>\n",
              "      <td>24.26629</td>\n",
              "      <td>-10.50788</td>\n",
              "      <td>-8.68412</td>\n",
              "      <td>54.75759</td>\n",
              "      <td>194.74034</td>\n",
              "      <td>7.95966</td>\n",
              "      <td>-18.22685</td>\n",
              "      <td>0.06463</td>\n",
              "      <td>-2.63069</td>\n",
              "      <td>26.02561</td>\n",
              "      <td>1.75729</td>\n",
              "      <td>-262.36917</td>\n",
              "      <td>-233.60089</td>\n",
              "      <td>-2.50502</td>\n",
              "      <td>-12.14279</td>\n",
              "      <td>81.37617</td>\n",
              "      <td>2.07554</td>\n",
              "      <td>-1.82381</td>\n",
              "      <td>183.65292</td>\n",
              "      <td>22.64797</td>\n",
              "      <td>-39.98887</td>\n",
              "      <td>43.37381</td>\n",
              "      <td>-31.56737</td>\n",
              "      <td>-4.88840</td>\n",
              "      <td>-36.53213</td>\n",
              "      <td>-23.94662</td>\n",
              "      <td>-84.19275</td>\n",
              "      <td>66.00518</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>-6.36304</td>\n",
              "      <td>6.63016</td>\n",
              "      <td>-3.35142</td>\n",
              "      <td>37.64085</td>\n",
              "      <td>2174.08189</td>\n",
              "      <td>697.43346</td>\n",
              "      <td>459.24587</td>\n",
              "      <td>742.78961</td>\n",
              "      <td>229.30783</td>\n",
              "      <td>387.89697</td>\n",
              "      <td>249.06662</td>\n",
              "      <td>245.89870</td>\n",
              "      <td>176.20527</td>\n",
              "      <td>98.82222</td>\n",
              "      <td>150.97286</td>\n",
              "      <td>78.49057</td>\n",
              "      <td>-62.00282</td>\n",
              "      <td>43.49659</td>\n",
              "      <td>-96.42719</td>\n",
              "      <td>-108.96608</td>\n",
              "      <td>14.22854</td>\n",
              "      <td>14.54178</td>\n",
              "      <td>-23.55608</td>\n",
              "      <td>-39.36953</td>\n",
              "      <td>-43.59209</td>\n",
              "      <td>20.83714</td>\n",
              "      <td>35.63919</td>\n",
              "      <td>-181.34947</td>\n",
              "      <td>-93.66614</td>\n",
              "      <td>-90.55616</td>\n",
              "      <td>...</td>\n",
              "      <td>4.56917</td>\n",
              "      <td>-37.32280</td>\n",
              "      <td>4.15159</td>\n",
              "      <td>12.24315</td>\n",
              "      <td>35.02697</td>\n",
              "      <td>-178.89573</td>\n",
              "      <td>82.46573</td>\n",
              "      <td>-20.49425</td>\n",
              "      <td>101.78577</td>\n",
              "      <td>-19.77808</td>\n",
              "      <td>-21.52657</td>\n",
              "      <td>3.36303</td>\n",
              "      <td>-11.63176</td>\n",
              "      <td>51.55411</td>\n",
              "      <td>-50.57576</td>\n",
              "      <td>-28.14755</td>\n",
              "      <td>-83.15795</td>\n",
              "      <td>-7.35260</td>\n",
              "      <td>-22.11505</td>\n",
              "      <td>1.18279</td>\n",
              "      <td>-122.70467</td>\n",
              "      <td>150.57360</td>\n",
              "      <td>24.37468</td>\n",
              "      <td>41.19821</td>\n",
              "      <td>-37.04318</td>\n",
              "      <td>-28.72986</td>\n",
              "      <td>162.19614</td>\n",
              "      <td>22.18309</td>\n",
              "      <td>-8.63509</td>\n",
              "      <td>85.23416</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>0.93609</td>\n",
              "      <td>1.60923</td>\n",
              "      <td>2.19223</td>\n",
              "      <td>47.32082</td>\n",
              "      <td>894.28471</td>\n",
              "      <td>809.86615</td>\n",
              "      <td>318.78559</td>\n",
              "      <td>435.04497</td>\n",
              "      <td>341.61467</td>\n",
              "      <td>334.30734</td>\n",
              "      <td>322.99589</td>\n",
              "      <td>190.61921</td>\n",
              "      <td>235.84715</td>\n",
              "      <td>96.89517</td>\n",
              "      <td>210.58870</td>\n",
              "      <td>5.60463</td>\n",
              "      <td>-199.63958</td>\n",
              "      <td>204.85812</td>\n",
              "      <td>-77.17695</td>\n",
              "      <td>-65.79741</td>\n",
              "      <td>-6.95097</td>\n",
              "      <td>-12.15262</td>\n",
              "      <td>-3.85410</td>\n",
              "      <td>20.68990</td>\n",
              "      <td>-20.30480</td>\n",
              "      <td>37.15045</td>\n",
              "      <td>11.20673</td>\n",
              "      <td>-124.09519</td>\n",
              "      <td>-295.98542</td>\n",
              "      <td>-33.31169</td>\n",
              "      <td>...</td>\n",
              "      <td>45.25506</td>\n",
              "      <td>10.42226</td>\n",
              "      <td>27.88782</td>\n",
              "      <td>-17.12676</td>\n",
              "      <td>-31.54772</td>\n",
              "      <td>-76.86293</td>\n",
              "      <td>41.17343</td>\n",
              "      <td>-138.32535</td>\n",
              "      <td>-53.96905</td>\n",
              "      <td>-21.30266</td>\n",
              "      <td>-24.87362</td>\n",
              "      <td>-2.46595</td>\n",
              "      <td>-4.05003</td>\n",
              "      <td>-56.51161</td>\n",
              "      <td>-34.56445</td>\n",
              "      <td>-5.07092</td>\n",
              "      <td>-47.75605</td>\n",
              "      <td>64.81513</td>\n",
              "      <td>-97.42948</td>\n",
              "      <td>-12.59418</td>\n",
              "      <td>55.23699</td>\n",
              "      <td>28.85657</td>\n",
              "      <td>54.53513</td>\n",
              "      <td>-31.97077</td>\n",
              "      <td>20.03279</td>\n",
              "      <td>-8.07892</td>\n",
              "      <td>-55.12617</td>\n",
              "      <td>26.58961</td>\n",
              "      <td>-10.27183</td>\n",
              "      <td>-30.64232</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.54767</td>\n",
              "      <td>0.31568</td>\n",
              "      <td>92.35066</td>\n",
              "      <td>22.38696</td>\n",
              "      <td>-25.51870</td>\n",
              "      <td>-19.04928</td>\n",
              "      <td>20.67345</td>\n",
              "      <td>-5.19943</td>\n",
              "      <td>3.63566</td>\n",
              "      <td>-4.69088</td>\n",
              "      <td>2.49578</td>\n",
              "      <td>-3.02468</td>\n",
              "      <td>7.69273</td>\n",
              "      <td>1004.95615</td>\n",
              "      <td>785.06709</td>\n",
              "      <td>591.99232</td>\n",
              "      <td>495.75332</td>\n",
              "      <td>291.38165</td>\n",
              "      <td>434.08355</td>\n",
              "      <td>291.55265</td>\n",
              "      <td>303.58860</td>\n",
              "      <td>216.12189</td>\n",
              "      <td>126.10703</td>\n",
              "      <td>147.28090</td>\n",
              "      <td>6.17712</td>\n",
              "      <td>-133.67424</td>\n",
              "      <td>8.69259</td>\n",
              "      <td>-13.76138</td>\n",
              "      <td>-52.72072</td>\n",
              "      <td>-24.00961</td>\n",
              "      <td>-10.82297</td>\n",
              "      <td>11.42039</td>\n",
              "      <td>32.10442</td>\n",
              "      <td>-26.26130</td>\n",
              "      <td>-4.98856</td>\n",
              "      <td>20.84154</td>\n",
              "      <td>-129.69145</td>\n",
              "      <td>-157.91059</td>\n",
              "      <td>-43.29252</td>\n",
              "      <td>...</td>\n",
              "      <td>45.07251</td>\n",
              "      <td>-6.20682</td>\n",
              "      <td>17.53456</td>\n",
              "      <td>43.66313</td>\n",
              "      <td>-4.80024</td>\n",
              "      <td>-33.62226</td>\n",
              "      <td>-15.01731</td>\n",
              "      <td>-33.74416</td>\n",
              "      <td>2.12072</td>\n",
              "      <td>-46.83225</td>\n",
              "      <td>19.99596</td>\n",
              "      <td>26.31683</td>\n",
              "      <td>0.51657</td>\n",
              "      <td>-285.36622</td>\n",
              "      <td>-52.26245</td>\n",
              "      <td>-13.00341</td>\n",
              "      <td>-9.41185</td>\n",
              "      <td>69.69108</td>\n",
              "      <td>33.54907</td>\n",
              "      <td>-11.39665</td>\n",
              "      <td>93.61103</td>\n",
              "      <td>163.39892</td>\n",
              "      <td>5.00362</td>\n",
              "      <td>15.23807</td>\n",
              "      <td>-13.48394</td>\n",
              "      <td>-6.37431</td>\n",
              "      <td>-37.31287</td>\n",
              "      <td>92.47370</td>\n",
              "      <td>-90.00149</td>\n",
              "      <td>47.25143</td>\n",
              "      <td>6.59753</td>\n",
              "      <td>-50.69577</td>\n",
              "      <td>26.02574</td>\n",
              "      <td>18.94430</td>\n",
              "      <td>-0.33730</td>\n",
              "      <td>6.09352</td>\n",
              "      <td>35.18381</td>\n",
              "      <td>5.00283</td>\n",
              "      <td>-11.02257</td>\n",
              "      <td>0.02263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.57546</td>\n",
              "      <td>33.17843</td>\n",
              "      <td>50.53517</td>\n",
              "      <td>11.55217</td>\n",
              "      <td>-27.24764</td>\n",
              "      <td>-8.78206</td>\n",
              "      <td>-12.04282</td>\n",
              "      <td>-9.53930</td>\n",
              "      <td>28.61811</td>\n",
              "      <td>8.25435</td>\n",
              "      <td>-0.43743</td>\n",
              "      <td>5.66265</td>\n",
              "      <td>11.07787</td>\n",
              "      <td>1080.98902</td>\n",
              "      <td>1230.78393</td>\n",
              "      <td>1301.63542</td>\n",
              "      <td>952.84686</td>\n",
              "      <td>783.02498</td>\n",
              "      <td>560.79536</td>\n",
              "      <td>696.19620</td>\n",
              "      <td>253.36266</td>\n",
              "      <td>316.92697</td>\n",
              "      <td>151.75689</td>\n",
              "      <td>144.07059</td>\n",
              "      <td>-3.02894</td>\n",
              "      <td>-111.65251</td>\n",
              "      <td>-56.64580</td>\n",
              "      <td>464.86598</td>\n",
              "      <td>150.52166</td>\n",
              "      <td>84.69609</td>\n",
              "      <td>-91.71196</td>\n",
              "      <td>89.31272</td>\n",
              "      <td>16.49867</td>\n",
              "      <td>-4.47074</td>\n",
              "      <td>-2.02539</td>\n",
              "      <td>13.27637</td>\n",
              "      <td>-153.73456</td>\n",
              "      <td>199.01552</td>\n",
              "      <td>-278.79072</td>\n",
              "      <td>...</td>\n",
              "      <td>0.05909</td>\n",
              "      <td>-92.07551</td>\n",
              "      <td>7.80480</td>\n",
              "      <td>-46.15966</td>\n",
              "      <td>-39.03309</td>\n",
              "      <td>32.52065</td>\n",
              "      <td>164.15989</td>\n",
              "      <td>-247.22025</td>\n",
              "      <td>-100.28773</td>\n",
              "      <td>-55.58712</td>\n",
              "      <td>8.38343</td>\n",
              "      <td>-4.57294</td>\n",
              "      <td>-20.08525</td>\n",
              "      <td>-357.00069</td>\n",
              "      <td>-232.78978</td>\n",
              "      <td>-112.81679</td>\n",
              "      <td>-66.16128</td>\n",
              "      <td>43.25003</td>\n",
              "      <td>18.48417</td>\n",
              "      <td>-2.50274</td>\n",
              "      <td>3.25927</td>\n",
              "      <td>94.57509</td>\n",
              "      <td>-24.31254</td>\n",
              "      <td>62.97582</td>\n",
              "      <td>-19.41809</td>\n",
              "      <td>10.35282</td>\n",
              "      <td>-91.89392</td>\n",
              "      <td>10.51922</td>\n",
              "      <td>-74.98521</td>\n",
              "      <td>12.29948</td>\n",
              "      <td>11.63681</td>\n",
              "      <td>25.44182</td>\n",
              "      <td>134.62382</td>\n",
              "      <td>21.51982</td>\n",
              "      <td>8.17570</td>\n",
              "      <td>35.46251</td>\n",
              "      <td>11.57736</td>\n",
              "      <td>4.50056</td>\n",
              "      <td>-4.62739</td>\n",
              "      <td>1.40192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.26892</td>\n",
              "      <td>8.97526</td>\n",
              "      <td>75.23158</td>\n",
              "      <td>24.04945</td>\n",
              "      <td>-16.02105</td>\n",
              "      <td>-14.09491</td>\n",
              "      <td>8.11871</td>\n",
              "      <td>-1.87566</td>\n",
              "      <td>7.46701</td>\n",
              "      <td>1.18189</td>\n",
              "      <td>1.46625</td>\n",
              "      <td>-6.34226</td>\n",
              "      <td>8.40470</td>\n",
              "      <td>1619.74629</td>\n",
              "      <td>1762.43083</td>\n",
              "      <td>714.83843</td>\n",
              "      <td>1115.90792</td>\n",
              "      <td>525.85703</td>\n",
              "      <td>576.66300</td>\n",
              "      <td>350.41961</td>\n",
              "      <td>315.44672</td>\n",
              "      <td>267.94363</td>\n",
              "      <td>198.28158</td>\n",
              "      <td>201.97524</td>\n",
              "      <td>17.14701</td>\n",
              "      <td>-517.47978</td>\n",
              "      <td>-11.33396</td>\n",
              "      <td>59.52820</td>\n",
              "      <td>-25.10400</td>\n",
              "      <td>23.48782</td>\n",
              "      <td>12.63888</td>\n",
              "      <td>40.85580</td>\n",
              "      <td>-3.72243</td>\n",
              "      <td>-16.97392</td>\n",
              "      <td>0.04284</td>\n",
              "      <td>59.45956</td>\n",
              "      <td>-191.92216</td>\n",
              "      <td>445.36223</td>\n",
              "      <td>-120.62667</td>\n",
              "      <td>...</td>\n",
              "      <td>63.71442</td>\n",
              "      <td>3.70732</td>\n",
              "      <td>-9.36662</td>\n",
              "      <td>-25.75461</td>\n",
              "      <td>35.16677</td>\n",
              "      <td>-33.17382</td>\n",
              "      <td>13.80469</td>\n",
              "      <td>-107.14403</td>\n",
              "      <td>140.04250</td>\n",
              "      <td>-124.16153</td>\n",
              "      <td>-18.33032</td>\n",
              "      <td>-9.99397</td>\n",
              "      <td>8.96011</td>\n",
              "      <td>-411.27991</td>\n",
              "      <td>-99.75061</td>\n",
              "      <td>-75.51735</td>\n",
              "      <td>-88.57128</td>\n",
              "      <td>22.90222</td>\n",
              "      <td>4.14618</td>\n",
              "      <td>-16.83238</td>\n",
              "      <td>142.14168</td>\n",
              "      <td>326.91932</td>\n",
              "      <td>-3.49405</td>\n",
              "      <td>7.58991</td>\n",
              "      <td>-32.60725</td>\n",
              "      <td>-4.44469</td>\n",
              "      <td>-56.48952</td>\n",
              "      <td>-31.19491</td>\n",
              "      <td>-32.75384</td>\n",
              "      <td>-52.97111</td>\n",
              "      <td>18.03989</td>\n",
              "      <td>-58.46192</td>\n",
              "      <td>-65.56438</td>\n",
              "      <td>46.99856</td>\n",
              "      <td>-4.09602</td>\n",
              "      <td>56.37650</td>\n",
              "      <td>-18.29975</td>\n",
              "      <td>-0.30633</td>\n",
              "      <td>3.98364</td>\n",
              "      <td>-3.72556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2001</td>\n",
              "      <td>49.75468</td>\n",
              "      <td>33.99581</td>\n",
              "      <td>56.73846</td>\n",
              "      <td>2.89581</td>\n",
              "      <td>-2.92429</td>\n",
              "      <td>-26.44413</td>\n",
              "      <td>1.71392</td>\n",
              "      <td>-0.55644</td>\n",
              "      <td>22.08594</td>\n",
              "      <td>7.43847</td>\n",
              "      <td>-0.03578</td>\n",
              "      <td>1.66534</td>\n",
              "      <td>46.02060</td>\n",
              "      <td>789.58109</td>\n",
              "      <td>607.78514</td>\n",
              "      <td>248.54603</td>\n",
              "      <td>288.44004</td>\n",
              "      <td>330.16459</td>\n",
              "      <td>356.34405</td>\n",
              "      <td>194.21792</td>\n",
              "      <td>194.68297</td>\n",
              "      <td>158.43568</td>\n",
              "      <td>104.05610</td>\n",
              "      <td>183.10695</td>\n",
              "      <td>-39.54223</td>\n",
              "      <td>-225.39832</td>\n",
              "      <td>78.57149</td>\n",
              "      <td>20.62209</td>\n",
              "      <td>-9.44189</td>\n",
              "      <td>15.94754</td>\n",
              "      <td>51.24174</td>\n",
              "      <td>-4.36054</td>\n",
              "      <td>17.60516</td>\n",
              "      <td>9.03638</td>\n",
              "      <td>16.69044</td>\n",
              "      <td>42.70944</td>\n",
              "      <td>-77.62818</td>\n",
              "      <td>46.71905</td>\n",
              "      <td>-109.71028</td>\n",
              "      <td>...</td>\n",
              "      <td>36.17475</td>\n",
              "      <td>3.44547</td>\n",
              "      <td>46.40983</td>\n",
              "      <td>-21.52684</td>\n",
              "      <td>-10.50872</td>\n",
              "      <td>-8.62787</td>\n",
              "      <td>55.23069</td>\n",
              "      <td>-41.74452</td>\n",
              "      <td>-20.24522</td>\n",
              "      <td>21.78558</td>\n",
              "      <td>-8.69348</td>\n",
              "      <td>-1.85679</td>\n",
              "      <td>3.60117</td>\n",
              "      <td>-53.94252</td>\n",
              "      <td>-54.66970</td>\n",
              "      <td>-15.14957</td>\n",
              "      <td>-46.61675</td>\n",
              "      <td>48.45009</td>\n",
              "      <td>2.77737</td>\n",
              "      <td>-7.80854</td>\n",
              "      <td>38.29390</td>\n",
              "      <td>120.35575</td>\n",
              "      <td>23.84978</td>\n",
              "      <td>14.87696</td>\n",
              "      <td>-41.76961</td>\n",
              "      <td>-21.05519</td>\n",
              "      <td>-45.79645</td>\n",
              "      <td>9.16222</td>\n",
              "      <td>-21.48052</td>\n",
              "      <td>-9.70822</td>\n",
              "      <td>18.70812</td>\n",
              "      <td>5.20391</td>\n",
              "      <td>-27.75192</td>\n",
              "      <td>17.22100</td>\n",
              "      <td>-0.85210</td>\n",
              "      <td>-15.67150</td>\n",
              "      <td>-26.36257</td>\n",
              "      <td>5.48708</td>\n",
              "      <td>-9.13495</td>\n",
              "      <td>6.08680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2007</td>\n",
              "      <td>45.17809</td>\n",
              "      <td>46.34234</td>\n",
              "      <td>-40.65357</td>\n",
              "      <td>-2.47909</td>\n",
              "      <td>1.21253</td>\n",
              "      <td>-0.65302</td>\n",
              "      <td>-6.95536</td>\n",
              "      <td>-12.20040</td>\n",
              "      <td>17.02512</td>\n",
              "      <td>2.00002</td>\n",
              "      <td>-1.87785</td>\n",
              "      <td>9.85499</td>\n",
              "      <td>25.59837</td>\n",
              "      <td>1905.18577</td>\n",
              "      <td>3676.09074</td>\n",
              "      <td>1976.85531</td>\n",
              "      <td>913.11216</td>\n",
              "      <td>1957.52415</td>\n",
              "      <td>955.98525</td>\n",
              "      <td>942.72667</td>\n",
              "      <td>439.85991</td>\n",
              "      <td>591.66138</td>\n",
              "      <td>493.40770</td>\n",
              "      <td>496.38516</td>\n",
              "      <td>33.94285</td>\n",
              "      <td>-255.90134</td>\n",
              "      <td>-762.28079</td>\n",
              "      <td>-66.10935</td>\n",
              "      <td>-128.02217</td>\n",
              "      <td>198.12908</td>\n",
              "      <td>-34.44957</td>\n",
              "      <td>176.00397</td>\n",
              "      <td>-140.80069</td>\n",
              "      <td>-22.56380</td>\n",
              "      <td>12.77945</td>\n",
              "      <td>193.30164</td>\n",
              "      <td>314.20949</td>\n",
              "      <td>576.29519</td>\n",
              "      <td>-429.58643</td>\n",
              "      <td>...</td>\n",
              "      <td>-17.48938</td>\n",
              "      <td>75.58779</td>\n",
              "      <td>93.29243</td>\n",
              "      <td>85.83507</td>\n",
              "      <td>47.13972</td>\n",
              "      <td>312.85482</td>\n",
              "      <td>135.50478</td>\n",
              "      <td>-32.47886</td>\n",
              "      <td>49.67063</td>\n",
              "      <td>-214.73180</td>\n",
              "      <td>-77.83503</td>\n",
              "      <td>-47.26902</td>\n",
              "      <td>7.58366</td>\n",
              "      <td>-352.56581</td>\n",
              "      <td>-36.15655</td>\n",
              "      <td>-53.39933</td>\n",
              "      <td>-98.60417</td>\n",
              "      <td>-82.37799</td>\n",
              "      <td>45.81588</td>\n",
              "      <td>-16.91676</td>\n",
              "      <td>18.35888</td>\n",
              "      <td>-315.68965</td>\n",
              "      <td>-3.14554</td>\n",
              "      <td>125.45269</td>\n",
              "      <td>-130.18808</td>\n",
              "      <td>-3.06337</td>\n",
              "      <td>42.26602</td>\n",
              "      <td>-9.04929</td>\n",
              "      <td>26.41570</td>\n",
              "      <td>23.36165</td>\n",
              "      <td>-4.36742</td>\n",
              "      <td>-87.55285</td>\n",
              "      <td>-70.79677</td>\n",
              "      <td>76.57355</td>\n",
              "      <td>-7.71727</td>\n",
              "      <td>3.26926</td>\n",
              "      <td>-298.49845</td>\n",
              "      <td>11.49326</td>\n",
              "      <td>-89.21804</td>\n",
              "      <td>-15.09719</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 91 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0         1         2         3   ...         87        88         89        90\n",
              "0  2001  49.94357  21.47114  73.07750  ...   68.40795  -1.82223  -27.46348   2.26327\n",
              "1  2001  48.73215  18.42930  70.32679  ...   70.49388  12.04941   58.43453  26.92061\n",
              "2  2001  50.95714  31.85602  55.81851  ... -115.00698  -0.05859   39.67068  -0.66345\n",
              "3  2001  48.24750  -1.89837  36.29772  ...  -72.08993   9.90558  199.62971  18.85382\n",
              "4  2001  50.97020  42.20998  67.09964  ...   51.76631   7.88713   55.66926  28.74903\n",
              "5  2001  50.54767   0.31568  92.35066  ...   35.18381   5.00283  -11.02257   0.02263\n",
              "6  2001  50.57546  33.17843  50.53517  ...   11.57736   4.50056   -4.62739   1.40192\n",
              "7  2001  48.26892   8.97526  75.23158  ...  -18.29975  -0.30633    3.98364  -3.72556\n",
              "8  2001  49.75468  33.99581  56.73846  ...  -26.36257   5.48708   -9.13495   6.08680\n",
              "9  2007  45.17809  46.34234 -40.65357  ... -298.49845  11.49326  -89.21804 -15.09719\n",
              "\n",
              "[10 rows x 91 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4wnRJT1778j"
      },
      "source": [
        "# делим вручную на train и test\n",
        "X = df.iloc[:, 1:].values\n",
        "y = df.iloc[:, 0].values\n",
        "\n",
        "train_size = 463715\n",
        "X_train = X[:train_size, :]\n",
        "y_train = y[:train_size]\n",
        "X_test = X[train_size:, :]\n",
        "y_test = y[train_size:]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrO7gHKYrATO"
      },
      "source": [
        "# функция фиксации seed для random\n",
        "def set_random_seed(seed):\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_386JE_o5gOd"
      },
      "source": [
        "## Задание 0. (0 баллов, но при невыполнении максимум за все задание &mdash; 0 баллов)\n",
        "\n",
        "Мы будем использовать RMSE как метрику качества. Для самого первого бейзлайна обучите `Ridge` регрессию из `sklearn`. Кроме того, посчитайте качество при наилучшем константном прогнозе."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otwuisa56MLr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "66e51a8d-4e86-4266-9fc2-b4c1b7de81fb"
      },
      "source": [
        "# фиксируем random seed\n",
        "set_random_seed(192)\n",
        "\n",
        "# нормализуем входные данные\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# создаем объект ridge регрессии\n",
        "clf = Ridge()\n",
        "\n",
        "# обучаем\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# предсказываем на тестовой выборке\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# считаем качество RMSE\n",
        "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "\n",
        "# выведем график распределения количества песен в в зависимости от десятилетия (взят с kaggle)\n",
        "data_ten_year_gap = df[0].apply(lambda year : year-(year%10))\n",
        "sns.countplot(y=data_ten_year_gap, data=df)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE:  9.51016306843388\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5c3bf064d0>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Uc4RdqiFobY",
        "outputId": "84679ee6-d822-49e4-83d3-31c270768462"
      },
      "source": [
        "# Посчитаем качество для лучшего константного прогноза\n",
        "# Для метрики RMSE лучшим константным прогнозом является среднее арифметическое значений целевой переменной\n",
        "#-------для train---------\n",
        "const_pred_train = np.mean(y_train)\n",
        "print(const_pred_train)\n",
        "y_train_const_array = np.full(train_size, const_pred_train)\n",
        "print(\"RMSE train error: \", np.sqrt(mean_squared_error(y_train, y_train_const_array)))\n",
        "#-------для test---------\n",
        "const_pred_test = np.mean(y_test)\n",
        "print(const_pred_test)\n",
        "y_test_const_array = np.full(y_test.shape[0], const_pred_test)\n",
        "print(\"RMSE test error: \", np.sqrt(mean_squared_error(y_test, y_test_const_array)))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1998.3860949074324\n",
            "RMSE train error:  10.939755150678016\n",
            "1998.4957582800698\n",
            "RMSE test error:  10.851909820717683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6ilBKYt6OdD"
      },
      "source": [
        "## Задание 1. (максимум 10 баллов)\n",
        "\n",
        "Реализуйте обучение и тестирование нейронной сети для предоставленного вам набора данных. Соотношение между полученным значением метрики на тестовой выборке и баллами за задание следующее:\n",
        "\n",
        "- $\\text{RMSE} \\le 9.00 $ &mdash; 4 балла\n",
        "- $\\text{RMSE} \\le 8.90 $ &mdash; 6 баллов\n",
        "- $\\text{RMSE} \\le 8.80 $ &mdash; 8 баллов\n",
        "- $\\text{RMSE} \\le 8.75 $ &mdash; 10 баллов\n",
        "\n",
        "Есть несколько правил, которых вам нужно придерживаться:\n",
        "\n",
        "- Весь пайплайн обучения должен быть написан на PyTorch. При этом вы можете пользоваться другими библиотеками (`numpy`, `sklearn` и пр.), но только для обработки данных. То есть как угодно трансформировать данные и считать метрики с помощью этих библиотек можно, а импортировать модели из `sklearn` и выбивать с их помощью требуемое качество &mdash; нельзя. Также нельзя пользоваться библиотеками, для которых сам PyTorch является зависимостью.\n",
        "\n",
        "- Мы никак не ограничиваем ваш выбор архитектуры модели, но скорее всего вам будет достаточно полносвязной нейронной сети.\n",
        "\n",
        "- Для обучения запрещается использовать какие-либо иные данные, кроме обучающей выборки.\n",
        "\n",
        "- Ансамблирование моделей запрещено.\n",
        "\n",
        "### Полезные советы:\n",
        "\n",
        "- Очень вряд ли, что у вас с первого раза получится выбить качество на 10 баллов, поэтому пробуйте разные архитектуры, оптимизаторы и значения гиперпараметров. В идеале при запуске каждого нового эксперимента вы должны менять что-то одно, чтобы точно знать, как этот фактор влияет на качество.\n",
        "\n",
        "- Тот факт, что мы занимаемся глубинным обучением, не означает, что стоит забывать про приемы, использующиеся в классическом машинном обучении. Так что обязательно проводите исследовательский анализ данных, отрисовывайте нужные графики и не забывайте про масштабирование и подбор гиперпараметров.\n",
        "\n",
        "- Вы наверняка столкнетесь с тем, что ваша нейронная сеть будет сильно переобучаться. Для нейросетей существуют специальные методы регуляризации, например, dropout ([статья](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)) и weight decay ([блогпост](https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd)). Они, разумеется, реализованы в PyTorch. Попробуйте поэкспериментировать с ними.\n",
        "\n",
        "- Если вы чего-то не знаете, не гнушайтесь гуглить. В интернете очень много полезной информации, туториалов и советов по глубинному обучению в целом и по PyTorch в частности. Но не забывайте, что за скатанный код без ссылки на источник придется ответить по всей строгости!\n",
        "\n",
        "- Если вы сразу реализуете обучение на GPU, то у вас будет больше времени на эксперименты, так как любые вычисления будут работать быстрее. Google Colab предоставляет несколько GPU-часов (обычно около 8-10) в сутки бесплатно.\n",
        "\n",
        "- Чтобы отладить код, можете обучаться на небольшой части данных или даже на одном батче. Если лосс на обучающей выборке не падает, то что-то точно идет не так!\n",
        "\n",
        "- Пользуйтесь утилитами, которые вам предоставляет PyTorch (например, Dataset и Dataloader). Их специально разработали для упрощения разработки пайплайна обучения.\n",
        "\n",
        "- Скорее всего вы захотите отслеживать прогресс обучения. Для создания прогресс-баров есть удобная библиотека `tqdm`.\n",
        "\n",
        "- Быть может, вы захотите, чтобы графики рисовались прямо во время обучения. Можете воспользоваться функцией [clear_output](http://ipython.org/ipython-doc/dev/api/generated/IPython.display.html#IPython.display.clear_output), чтобы удалять старый график и рисовать новый на его месте.\n",
        "\n",
        "**ОБЯЗАТЕЛЬНО** рисуйте графики зависимости лосса/метрики на обучающей и тестовой выборках в зависимости от времени обучения. Если обучение занимает относительно небольшое число эпох, то лучше рисовать зависимость от номера шага обучения, если же эпох больше, то рисуйте зависимость по эпохам. Если проверяющий не увидит такого графика для вашей лучшей модели, то он в праве снизить баллы за задание.\n",
        "\n",
        "**ВАЖНО!** Ваше решение должно быть воспроизводимым. Если это не так, то проверяющий имеет право снизить баллы за задание. Чтобы зафиксировать random seed, воспользуйтесь функцией из ячейки ниже.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZW0gMe3vT8u"
      },
      "source": [
        "Вы можете придерживаться любой адекватной струкуры кода, но мы советуем воспользоваться следующими сигнатурами функций. Лучше всего, если вы проверите ваши предсказания ассертом: так вы убережете себя от разных косяков, например, что вектор предсказаний состоит из всего одного числа. В любом случае, внимательно следите за тем, для каких тензоров вы считаете метрику RMSE. При случайном или намеренном введении в заблуждение проверяющие очень сильно разозлятся."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8XOex36EC-1"
      },
      "source": [
        "# Источники:\n",
        "Далее я пользовался частично кодом с сайтов, а также некоторыми функциями с семинаров.\n",
        "\n",
        "#https://qudata.com/ml/ru/NN_Base_Torch_NN.html\n",
        "#https://towardsdatascience.com/pytorch-tabular-regression-428e9c9ac93"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNxkTwPrEeVR"
      },
      "source": [
        "# нормализуем также выходы относительно максимального значения выхода (видимо относительно 2011 года)\n",
        "y_scaler = max(y)\n",
        "y_train = y_train / y_scaler\n",
        "y_test = y_test / y_scaler"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5zliD3L5Ht9"
      },
      "source": [
        "# класс для подготовки датасета \n",
        "class RegressionDataset(Dataset):\n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPJjxg8nyAJo"
      },
      "source": [
        "# задаем константы \n",
        "batch_size = 64\n",
        "epoches = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "# делаем наборы данных\n",
        "train_set = RegressionDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
        "test_set = RegressionDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())\n",
        "\n",
        "# загружаем в dataloader\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_set, \n",
        "    batch_size=batch_size, \n",
        "    shuffle=True\n",
        "    )\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_set, \n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        "    )"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC9zpLoBsLAm"
      },
      "source": [
        "# функция отрисовки графика истории обучения (из ноутбука одного из семинаров)\n",
        "def plot_history(history, name, title=\"loss\"):\n",
        "    plt.figure()\n",
        "    plt.title('{}'.format(title))\n",
        "    plt.plot(history, label=\"{}\".format(name), zorder=1)\n",
        "    plt.xlabel(\"Steps\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hdw9s3zxWkOO"
      },
      "source": [
        "# задаем класс нейронной сети, прописываем ее архитектуру  \n",
        "class MultipleRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "      set_random_seed(192)\n",
        "      super(MultipleRegression, self).__init__()\n",
        "      # num_features - 90\n",
        "      self.relu = nn.LeakyReLU()\n",
        "      self.dropout_1 = nn.Dropout(0.15)\n",
        "      self.dropout_2 = nn.Dropout(0.15)\n",
        "      self.layer_1_bn=nn.BatchNorm1d(128)\n",
        "      self.layer_2_bn=nn.BatchNorm1d(16)\n",
        "      self.layer_1 = nn.Linear(90, 128)\n",
        "      self.layer_2 = nn.Linear(128, 16)\n",
        "      self.layer_out = nn.Linear(16, 1)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "      x = self.relu(self.layer_1_bn(self.layer_1(inputs)))\n",
        "      x = self.dropout_1(x)\n",
        "      x = self.relu(self.layer_2_bn(self.layer_2(x)))\n",
        "      x = self.dropout_2(x)\n",
        "      x = self.layer_out(x)\n",
        "      return (x)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultipleRegression()\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0003)\n",
        "criterion = nn.MSELoss()\n",
        "# неудачные пробы различных оптимизаторов\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.89, weight_decay=1e-4)\n",
        "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Wmxrf5Qveux",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e777d174-dc63-4d85-e9b7-f7ee8e93e3db"
      },
      "source": [
        "# функция обучения модели и вывода графика истории обучения\n",
        "epoches = 30\n",
        "def train(model, epochs, optimizer, criterion, train_loader, test_loader, device):\n",
        "    '''\n",
        "    params:\n",
        "        model - torch.nn.Module to be fitted\n",
        "        optimizer - model optimizer\n",
        "        criterion - loss function from torch.nn\n",
        "        train_loader - torch.utils.data.Dataloader with train set\n",
        "        test_loader - torch.utils.data.Dataloader with test set\n",
        "                      (if you wish to validate during training)\n",
        "    '''\n",
        "    all_train_losses = []\n",
        "    validation_errors = []\n",
        "    for e in tqdm(range(1, epoches+1)):\n",
        "      train_loss_rmse = []\n",
        "      model.train()\n",
        "      for X_train_batch, y_train_batch in train_loader:\n",
        "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_train_pred = model(X_train_batch)\n",
        "        train_loss = criterion(y_train_pred.float(), y_train_batch.unsqueeze(1).float())\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss_rmse.append(torch.sqrt(train_loss).detach().numpy())\n",
        "      RMSE_scaled_train_loss = np.mean(train_loss_rmse) * y_scaler\n",
        "      all_train_losses.append(RMSE_scaled_train_loss)\n",
        "      print('Train loss:',  RMSE_scaled_train_loss)\n",
        "    name = \"train_full_plot\"\n",
        "    plot_history(all_train_losses, name)\n",
        "    name = \"train_cutted\" # без первых двух записей ошибок, так как они очень большие и дальше не видно как падает ошибка \n",
        "    plot_history(all_train_losses[2:], name)\n",
        "    return all_train_losses\n",
        "\n",
        "\n",
        "print(\"Training started\")\n",
        "train(model, epoches, optimizer, criterion, train_loader, test_loader, device)\n",
        "print(\"Train success\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 1/30 [00:23<11:09, 23.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 445.64082849025726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 2/30 [00:46<10:53, 23.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 62.42574723996222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 3/30 [01:09<10:27, 23.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 9.913890646770597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 4/30 [01:33<10:10, 23.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 9.185323444195092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 5/30 [01:58<09:57, 23.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.994259665254503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 6/30 [02:22<09:37, 24.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.920016444753855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 7/30 [02:47<09:16, 24.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.886155535466969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 8/30 [03:11<08:55, 24.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.86665594438091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 9/30 [03:36<08:32, 24.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.838773299008608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 10/30 [04:00<08:08, 24.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.814825594890863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 11/30 [04:25<07:44, 24.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.793728428892791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 12/30 [04:49<07:19, 24.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.783173759002239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 13/30 [05:13<06:54, 24.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.773521821945906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 14/30 [05:38<06:31, 24.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.745559578761458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 15/30 [06:02<06:06, 24.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.73936218675226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 16/30 [06:27<05:44, 24.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.733669538516551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 17/30 [06:52<05:19, 24.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.723639277741313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 18/30 [07:16<04:54, 24.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.717940074391663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 19/30 [07:41<04:29, 24.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.706136187072843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 20/30 [08:05<04:03, 24.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.692180349491537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 21/30 [08:28<03:36, 24.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.69336401578039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 22/30 [08:51<03:10, 23.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.691640020813793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 23/30 [09:15<02:45, 23.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.682096711359918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 24/30 [09:37<02:20, 23.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.673203294631094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 25/30 [10:00<01:56, 23.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.679491521790624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 26/30 [10:23<01:32, 23.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.663018520455807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 27/30 [10:46<01:09, 23.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.669407883659005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 28/30 [11:09<00:46, 23.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.657556237652898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 29/30 [11:32<00:23, 23.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.653612868394703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [11:55<00:00, 23.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 8.653946242760867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWP9cA-C9QBJ"
      },
      "source": [
        "def test(model, criterion, test_loader, device):\n",
        "    '''\n",
        "    params:\n",
        "        model - torch.nn.Module to be evaluated on test set\n",
        "        criterion - loss function from torch.nn\n",
        "        test_loader - torch.utils.data.Dataloader with test set\n",
        "    ----------\n",
        "    returns:\n",
        "        predicts - torch.tensor with shape (len(test_loader.dataset), ),\n",
        "                   which contains predictions for test objects\n",
        "    '''\n",
        "    y_pred_list = []\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      for X_batch_test, y_batch_test in test_loader:\n",
        "        X_batch_test, y_batch_test = X_batch_test.to(device), y_batch_test.to(device)\n",
        "        y_pred = model(X_batch_test) * y_scaler\n",
        "        y_test = y_batch_test.unsqueeze(1).float() * y_scaler\n",
        "        y_pred = y_pred.clone().detach()\n",
        "        y_pred_list.append(y_pred)\n",
        "    predicts = torch.cat(y_pred_list)\n",
        "    return predicts"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngzbg9qB3anL"
      },
      "source": [
        "## Итоговая ошибка RMSE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dsIbMt-B5U-",
        "outputId": "75faf160-0403-4811-e394-26d1c68adc23"
      },
      "source": [
        "# смотрим качество по RMSE\n",
        "y_pred_result_test = test(model, criterion, test_loader, device)\n",
        "print(np.sqrt(mean_squared_error(y_pred_result_test, y_test * y_scaler)))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.761421179782824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PVEXDgD6Swj"
      },
      "source": [
        "assert test(model, criterion, test_loader, device).shape[0] == y_test.shape[0]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bine9EES6TIn"
      },
      "source": [
        "## Задание 2. (0 баллов, но при невыполнении максимум за все задание &mdash; 0 баллов)\n",
        "\n",
        "Напишите небольшой отчет о том, как вы добились полученного качества: какие средства использовали и какие эксперименты проводили. Подробно расскажите об архитектурах и значениях гиперпараметров, а также какие метрики на тесте они показывали. Чтобы отчет был зачтен, необходимо привести хотя бы 3 эксперимента."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma8TV88V4vFM"
      },
      "source": [
        "# Эксперимент первый:\n",
        "При подготовке датасета нормализовал входные данные с помощью MinMaxScaler. Батчи взял по 128. \n",
        "С самого начала решил не делать много слоев и ограничиться 3мя полносвязными. В качестве функции активации взял ReLU. В результате получилась сеть (90, 32) -> ReLU -> (16, 16) -> ReLU -> (16, 1). В конце единица, так как предсказываем единственную переменную -- год выхода песни. Оптимизатор использовал SGD и изменял там только learning_rate, однако данный набор средств не позволил получить ошибку RMSE на тесте меньше 15.\n",
        "\n",
        "# Эксперимент второй\n",
        "Во время второго эксперимента решил использовать больше нейронов и применить к каждому слою batchnorm, получил сеть \"пошире\": (90, 96) -> LeakyReLU -> (64, 32) -> LeakyReLU -> (32, 1), в которой число нейронов убывает слева направо. Также вместо обычного ReLU взял LeakyReLU, у которого при  x < 0, не ноль, а небольшое отрицательное значение (угловой коэффициент около 0,01), что помогает бороться с затухающим градиентом. Добавил параметры SGD weight_decay - регуляризатор, чтобы уберечь модель от переобучения под тестовую выборку и momentum (импульс) для корректировки точности шага обучения.\n",
        "Далее опять начал перебирать параметры weight_decacy и learning_rate, а также batch_size. Уже более глубоко изучил документацию и пошерстил форумы, какие значения в каких зачах наиболее оптимальны. Благодаря более глубокому анализу теоретической части и увеличению количества гиперпараметров, пришел к следующему: learning_rate = 0,001, momentum = 0.89, weight_decay = 1e-4, batch_size = 128. \n",
        "При такой архитектуре удалось добиться качества 8.966 (ура, 4 балла как минимум в кармане). Но стало ясно, что при данной архитектуре и методах предобработки данных  как ни крути гиперпараметры, сильно меньше качество не получишь( Но тут пришла подсказка из чата в канале курса в телеграмме!!! И об этом будет третий эксперимент.\n",
        "\n",
        "# Эксперимент третий\n",
        "Действовать решил точно по подсказке -- ключом к успеху была нормировка выходных данных. Скейлить решил вручную -- взял самое большое значение \"игрек\" и на него поделил y_train и y_test. Потом в функциях train и test применил обратное масштабирование (домножил на scale), чтобы корректно считалось RMSE. Архитектуру самой нейросети оставил прежней -- три полносвязных слоя, лишь немного поменял количество нейронов (90, 128) -> LeakyReLU -> (128, 16) -> LeakyReLU -> (16, 1). По совету из телеграмма заменил SGD на Adam с learning_rate = 0.0003. Увеличил количество эпох до 30 и вот заветные 8.76 RMSE успешно показались в ячейке вывода программы. "
      ]
    }
  ]
}